import { useEffect, useRef, useState } from 'react';
import { FaceLandmarker, FilesetResolver } from '@mediapipe/tasks-vision';
import { Face } from 'kalidokit';

export interface FacePose {
    head: {
        x: number;
        y: number;
        z: number;
    };
    eye: {
        l: number;
        r: number;
    };
    mouth: {
        x: number;
        y: number;
        shape: {
            A: number;
            E: number;
            I: number;
            O: number;
            U: number;
        };
    };
    brow: number;
    pupil: {
        x: number;
        y: number;
    };
}

export function useFaceTracking(enabled: boolean = false) {
    const [facePose, setFacePose] = useState<FacePose | null>(null);
    const [isReady, setIsReady] = useState(false);
    const [error, setError] = useState<string | null>(null);

    const videoRef = useRef<HTMLVideoElement | null>(null);
    const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
    const animationFrameRef = useRef<number | null>(null);
    const streamRef = useRef<MediaStream | null>(null);

    // MediaPipe Ï¥àÍ∏∞Ìôî
    useEffect(() => {
        if (!enabled) return;

        let mounted = true;

        const initializeFaceLandmarker = async () => {
            try {
                console.log('üé• MediaPipe FaceLandmarker Ï¥àÍ∏∞Ìôî Ï§ë...');

                const vision = await FilesetResolver.forVisionTasks(
                    'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm'
                );

                const faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
                    baseOptions: {
                        modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task',
                        delegate: 'GPU'
                    },
                    runningMode: 'VIDEO',
                    numFaces: 1,
                    minFaceDetectionConfidence: 0.5,
                    minFacePresenceConfidence: 0.5,
                    minTrackingConfidence: 0.5,
                    outputFaceBlendshapes: true,
                    outputFacialTransformationMatrixes: true
                });

                if (mounted) {
                    faceLandmarkerRef.current = faceLandmarker;
                    setIsReady(true);
                    console.log('‚úÖ FaceLandmarker Ï¥àÍ∏∞Ìôî ÏôÑÎ£å');
                }
            } catch (err) {
                console.error('‚ùå FaceLandmarker Ï¥àÍ∏∞Ìôî Ïã§Ìå®:', err);
                if (mounted) {
                    setError('Face tracking Ï¥àÍ∏∞Ìôî Ïã§Ìå®');
                }
            }
        };

        initializeFaceLandmarker();

        return () => {
            mounted = false;
            if (faceLandmarkerRef.current) {
                faceLandmarkerRef.current.close();
                faceLandmarkerRef.current = null;
            }
        };
    }, [enabled]);

    // ÏõπÏ∫† Ïä§Ìä∏Î¶º ÏãúÏûë
    useEffect(() => {
        if (!enabled || !isReady) return;

        let mounted = true;

        const startWebcam = async () => {
            try {
                console.log('üé• ÏõπÏ∫† Ïä§Ìä∏Î¶º ÏãúÏûë Ï§ë...');

                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: { ideal: 640 },
                        height: { ideal: 480 },
                        frameRate: { ideal: 30 }
                    }
                });

                if (!mounted) {
                    stream.getTracks().forEach(track => track.stop());
                    return;
                }

                streamRef.current = stream;

                if (videoRef.current) {
                    videoRef.current.srcObject = stream;
                    videoRef.current.play();
                    console.log('‚úÖ ÏõπÏ∫† Ïä§Ìä∏Î¶º ÏãúÏûë ÏôÑÎ£å');

                    // ÎπÑÎîîÏò§Í∞Ä Ï§ÄÎπÑÎêòÎ©¥ Ï∂îÏ†Å ÏãúÏûë
                    videoRef.current.onloadedmetadata = () => {
                        if (mounted && videoRef.current) {
                            startTracking();
                        }
                    };
                }
            } catch (err) {
                console.error('‚ùå ÏõπÏ∫† Ï†ëÍ∑º Ïã§Ìå®:', err);
                if (mounted) {
                    setError('ÏõπÏ∫† Ï†ëÍ∑º Í∂åÌïúÏù¥ ÌïÑÏöîÌï©ÎãàÎã§');
                }
            }
        };

        startWebcam();

        return () => {
            mounted = false;
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
                streamRef.current = null;
            }
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
                animationFrameRef.current = null;
            }
        };
    }, [enabled, isReady]);

    // ÏñºÍµ¥ Ï∂îÏ†Å Î£®ÌîÑ
    const startTracking = () => {
        if (!videoRef.current || !faceLandmarkerRef.current) return;

        const video = videoRef.current;
        const faceLandmarker = faceLandmarkerRef.current;

        const detectFace = () => {
            if (!video || video.readyState !== 4 || !faceLandmarker) {
                animationFrameRef.current = requestAnimationFrame(detectFace);
                return;
            }

            const startTimeMs = performance.now();
            const results = faceLandmarker.detectForVideo(video, startTimeMs);

            if (results.faceLandmarks && results.faceLandmarks.length > 0) {
                const landmarks = results.faceLandmarks[0];

                // KalidokitÏúºÎ°ú Live2D ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞
                const riggedFace = Face.solve(landmarks, {
                    runtime: 'mediapipe',
                    video: video
                });

                if (riggedFace) {
                    setFacePose({
                        head: {
                            x: riggedFace.head.x,
                            y: riggedFace.head.y,
                            z: riggedFace.head.z
                        },
                        eye: {
                            l: riggedFace.eye.l,
                            r: riggedFace.eye.r
                        },
                        mouth: {
                            x: riggedFace.mouth.x,
                            y: riggedFace.mouth.y,
                            shape: {
                                A: riggedFace.mouth.shape.A || 0,
                                E: riggedFace.mouth.shape.E || 0,
                                I: riggedFace.mouth.shape.I || 0,
                                O: riggedFace.mouth.shape.O || 0,
                                U: riggedFace.mouth.shape.U || 0
                            }
                        },
                        brow: riggedFace.brow,
                        pupil: {
                            x: riggedFace.pupil.x,
                            y: riggedFace.pupil.y
                        }
                    });
                }
            }

            animationFrameRef.current = requestAnimationFrame(detectFace);
        };

        detectFace();
    };

    return {
        facePose,
        isReady,
        error,
        videoRef
    };
}
